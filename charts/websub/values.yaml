## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry and imagePullSecrets
##
# global:
#   imageRegistry: myRegistryName
#   imagePullSecrets:
#     - myRegistryKeySecretName
#   storageClass: myStorageClass
global:
  keycloakBaseUrl: 'https://keycloak.your.org'

hostname: websub.openg2p.sandbox.net

## Add labels to all the deployed resources
##
commonLabels: {}

## Add annotations to all the deployed resources
##
commonAnnotations: {}

## Extra objects to deploy (value evaluated as a template)
##
extraDeploy: []

## Number of nodes
##
replicaCount: 1

service:
  type: ClusterIP
  port: 80
  ## loadBalancerIP for the SuiteCRM Service (optional, cloud specific)
  ## ref: http://kubernetes.io/docs/user-guide/services/#type-loadbalancer
  ##
  ## loadBalancerIP:
  ##
  ## nodePorts:
  ##   http: <to set explicitly, choose port between 30000-32767>
  ##   https: <to set explicitly, choose port between 30000-32767>
  ##

  nodePorts:
    http: ""
    https: ""
  ## Enable client source IP preservation
  ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
  ##
  externalTrafficPolicy: Cluster

image:
  registry: docker.io
  repository: openg2p/websub-service
  tag: 1.2.0.1
  ## Specify a imagePullPolicy
  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
  ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  pullPolicy: Always
  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ##
  # pullSecrets:
  #   - myRegistryKeySecretName

## Port on which this particular spring service module is running.
containerPort: 9191

## Configure extra options for liveness and readiness probes
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
##
startupProbe:
  enabled: true
  httpGet:
    path: /hub/actuator/health
    port: http
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 30
  successThreshold: 1

livenessProbe:
  enabled: true
  httpGet:
    path: /hub/actuator/health
    port: http
  initialDelaySeconds: 20
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1

readinessProbe:
  enabled: true
  httpGet:
    path: /hub/actuator/health
    port: http
  initialDelaySeconds: 0
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1

## Command and args for running the container (set to default if not set). Use array form
##
command: []
args: []

## Deployment pod host aliases
## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
##
hostAliases: []

## ref: http://kubernetes.io/docs/user-guide/compute-resources/
##
## We usually recommend not to specify default resources and to leave this as a conscious
## choice for the user. This also increases chances charts run on environments with little
## resources, such as Minikube. If you do want to specify resources, uncomment the following
## lines, adjust them as necessary.
# resources:
#   limits:
#     cpu: 500m
#     memory: 2250Mi
#   requests:
#     cpu: 100m
#     memory: 1500Mi
resources: {}

## Specify any JAVA_OPTS string here. These typically will be specified in conjunction with above resources. Example:
# javaOpts: "-Xms500M -Xmx500M"
javaOpts: ""

## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
## Clamav container already runs as 'mosip' user, so we may not need to enable this
containerSecurityContext:
  enabled: false
  runAsUser: 1001
  runAsNonRoot: true

## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
##
podSecurityContext:
  enabled: false
  fsGroup: 1001

## Pod affinity preset
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
## Allowed values: soft, hard
##
podAffinityPreset: ""

## Pod anti-affinity preset
## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
## Allowed values: soft, hard
##
podAntiAffinityPreset: soft

## Node affinity preset
## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
## Allowed values: soft, hard
##
nodeAffinityPreset:
  ## Node affinity type
  ## Allowed values: soft, hard
  ##
  type: ""
  ## Node label key to match
  ## E.g.
  ## key: "kubernetes.io/e2e-az-name"
  ##
  key: ""
  ## Node label values to match
  ## E.g.
  ## values:
  ##   - e2e-az1
  ##   - e2e-az2
  ##
  values: []

## Affinity for pod assignment. Evaluated as a template.
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
##
affinity: {}

## Node labels for pod assignment. Evaluated as a template.
## ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}

## Tolerations for pod assignment. Evaluated as a template.
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations: []

## Pod extra labels
## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
##
podLabels: {}

## Annotations for server pods.
## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
##
podAnnotations: {}

##  pods' priority.
## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
##
# priorityClassName: ""

## lifecycleHooks for the  container to automate configuration before or after startup.
##
lifecycleHooks: {}

## Custom Liveness probes for
##
customLivenessProbe: {}

## Custom Rediness probes
##
customReadinessProbe: {}

## Update strategy - only really applicable for deployments with RWO PVs attached
## If replicas = 1, an update can get "stuck", as the previous pod remains attached to the
## PV, and the "incoming" pod can never start. Changing the strategy to "Recreate" will
## terminate the single previous pod, so that the new, incoming pod can attach to the PV
##
updateStrategy:
  type: RollingUpdate

## Add init containers to the  pods.
## Example:
## initContainers:
##   - name: your-image-name
##     image: your-image
##     imagePullPolicy: Always
##     ports:
##       - name: portname
##         containerPort: 1234
##
initContainers: []

## Add sidecars to the  pods.
## Example:
## sidecars:
##   - name: your-image-name
##     image: your-image
##     imagePullPolicy: Always
##     ports:
##       - name: portname
##         containerPort: 1234
##
sidecars: []

## Specifies whether a ServiceAccount should be created
##
serviceAccount:
  create: true
  ## The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the fullname template
  ##
  name:

## Prometheus Metrics
##
metrics:
  enabled: false
  ## Prometheus pod annotations
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations:
    prometheus.io/scrape: "true"

  endpointPath: /v1/websub/actuator/prometheus

  ## Prometheus Service Monitor
  ## ref: https://github.com/coreos/prometheus-operator
  ##
  serviceMonitor:
    ## If the operator is installed in your cluster, set to true to create a Service Monitor Entry
    ##
    enabled: true
    ## Specify the namespace in which the serviceMonitor resource will be created
    ##
    # namespace: ""
    ## Specify the interval at which metrics should be scraped
    ##
    interval: 10s
    ## Specify the timeout after which the scrape is ended
    ##
    # scrapeTimeout: 30s
    ## Specify Metric Relabellings to add to the scrape endpoint
    ##
    # relabellings:
    ## Specify honorLabels parameter to add the scrape endpoint
    ##
    honorLabels: false
    ## Used to pass Labels that are used by the Prometheus installed in your cluster to select Service Monitors to work with
    ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
    ##
    additionalLabels: {}

  ## Custom PrometheusRule to be defined
  ## The value is evaluated as a template, so, for example, the value can depend on .Release or .Chart
  ## ref: https://github.com/coreos/prometheus-operator#customresourcedefinitions
  ##
  prometheusRule:
    enabled: false
    additionalLabels: {}
    namespace: ''
    ## List of rules, used as template by Helm.
    ## These are just examples rules inspired from https://awesome-prometheus-alerts.grep.to/rules.html
    # rules:
    #   - alert: RabbitmqDown
    #     expr: rabbitmq_up{service="{{ template "rabbitmq.fullname" . }}"} == 0
    #     for: 5m
    #     labels:
    #       severity: error
    rules: []

## Extra volumes to add to the deployment
##
extraVolumes: []

## Extra volume mounts to add to the container
##
extraVolumeMounts: []

istio:
  enabled: true
  virtualservice:
    enabled: true
    host: ""
    gateway: "internal"
    destination: '{{ include "common.names.fullname" . }}'
    destinationPort: '{{ .Values.service.port }}'
    prefix: /hub
    rewriteUri: ""
  gateway:
    enabled: false
    host: ""
    ingressController:
      istio: ingressgateway
    httpTlsRedirect: false
    httpEnabled: true
    httpsEnabled: false
    tls:
      mode: SIMPLE
      credentialName: ""

kafka:
  enabled: true
  nameOverride: websub-kafka
  heapOpts: ""
  extraConfig: |-
    default.replication.factor=1
    offsets.topic.replication.factor=1
    transaction.state.log.replication.factor=1
    auto.create.topics.enable=true
    delete.topic.enable=true
    num.partitions=3
  controller:
    heapOpts: ""
    replicaCount: 1
    resourcesPreset: "none"
  broker:
    heapOpts: ""
    persistence:
      enabled: false
  listeners:
    client:
      protocol: PLAINTEXT
    controller:
      protocol: PLAINTEXT
    interbroker:
      protocol: PLAINTEXT
    external:
      protocol: PLAINTEXT
  service:
    ports:
      client: 9092

consolidator:
  enabled: true
  replicaCount: 1
  service:
    type: ClusterIP
    port: 80
    nodePorts:
      http: ""
      https: ""
    externalTrafficPolicy: Cluster
  image:
    registry: docker.io
    repository: openg2p/consolidator-websub-service
    tag: 1.2.0.1
    pullPolicy: Always
    pullSecrets: []
    #   - myRegistryKeySecretName
  containerPort: 9192
  startupProbe:
    enabled: true
    httpGet:
      path: /consolidator/actuator/health
      port: http
    initialDelaySeconds: 0
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 30
    successThreshold: 1
  livenessProbe:
    enabled: true
    httpGet:
      path: /consolidator/actuator/health
      port: http
    initialDelaySeconds: 20
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  readinessProbe:
    enabled: true
    httpGet:
      path: /consolidator/actuator/health
      port: http
    initialDelaySeconds: 0
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  command: []
  args: []
  hostAliases: []
  resources: {}
  javaOpts: ""
  containerSecurityContext:
    enabled: false
    runAsUser: 1001
    runAsNonRoot: true
  podSecurityContext:
    enabled: false
    fsGroup: 1001
  podAffinityPreset: ""
  podAntiAffinityPreset: soft
  nodeAffinityPreset:
    type: ""
    key: ""
    values: []
  affinity: {}
  nodeSelector: {}
  tolerations: []
  podLabels:
    app.kubernetes.io/name: '{{ include "common.names.name" . }}-consolidator'
  podAnnotations: {}
  # priorityClassName: ""
  lifecycleHooks: {}
  updateStrategy:
    type: RollingUpdate
  initContainers: []
  sidecars: []
  extraVolumes: []
  extraVolumeMounts: []
  istio:
    enabled: true
    virtualservice:
      enabled: true
      host: ""
      gateway: "internal"
      destination: '{{ include "common.names.fullname" . }}-consolidator'
      destinationPort: '{{ .Values.consolidator.service.port }}'
      prefix: /consolidator
      rewriteUri: ""
    gateway:
      enabled: false
      host: ""
      ingressController:
        istio: ingressgateway
      httpTlsRedirect: false
      httpEnabled: true
      httpsEnabled: false
      tls:
        mode: SIMPLE
        credentialName: ""
  envVars:
    JDK_JAVA_OPTIONS: '{{ tpl .Values.consolidator.javaOpts $ }}'
    CONSOLIDATOR_PORT: '{{ .Values.consolidator.containerPort }}'
    KAFKA_BOOTSTRAP_HOSTNAME: '{{ tpl .Values.kafkaInstallationName $ }}'
    KAFKA_PORT: '9092'
    KAFKA_BOOTSTRAP_SERVER: '{{ tpl .Values.envVars.KAFKA_BOOTSTRAP_HOSTNAME $ }}:{{ tpl .Values.envVars.KAFKA_PORT $ }}'

  envVarsFrom: {}

  configTomlMountPath: /etc/config.template.toml

  configToml: |-
    [consolidatorService.config]
    # IP and Port of the Kafka bootstrap node
    KAFKA_BOOTSTRAP_NODE = "${KAFKA_BOOTSTRAP_SERVER}"

    # Kafka topic which will get notified for websub topic registration/deregistration
    # All the hubs must be pointed to the same Kafka topic to notify websub topic registration/deregistration
    REGISTERED_WEBSUB_TOPICS_TOPIC = "registered-websub-topics"

    # Kafka topic which stores consolidated websub topics for the hub
    CONSOLIDATED_WEBSUB_TOPICS_TOPIC = "consolidated-websub-topics"

    # Kafka topic which will get notified for websub subscription/unsubscription
    # All the hubs must be pointed to the same Kafka topic to notify websub subscription/unsubscription
    WEBSUB_SUBSCRIBERS_TOPIC = "registered-websub-subscribers"

    # Kafka topic which is stores consolidated websub subscribers for this server
    CONSOLIDATED_WEBSUB_SUBSCRIBERS_TOPIC = "consolidated-websub-subscribers"

    # The interval in which Kafka consumers wait for new messages
    POLLING_INTERVAL = 10.0

    # The period in which Kafka close method waits to complete
    GRACEFUL_CLOSE_PERIOD = 5.0

    # The disk space threshold for healthcheck
    DISK_SPACE_THRESHOLD = 10485760

    # The port that is used to start the consolidator
    CONSOLIDATOR_PORT = ${CONSOLIDATOR_PORT}

    # consolidator health endpoint
    CONSOLIDATOR_HEALTH_ENDPOINT = "/consolidator/actuator/health"

    [ballerina.http.accessLogConfig]
    # Enable printing access logs in console
    console = true              # Default is false

    [ballerina.http.traceLogAdvancedConfig]
    # Enable printing trace logs in console
    console = false              # Default is false

  startUpCommand: |-
    #!/usr/bin/env bash
    echo "==> Copying Configs"
    envsubst < {{ tpl .Values.consolidator.configTomlMountPath $ }} > Config.toml
    echo "==> Waiting for kafka"
    RETRY_COUNT=0
    while :; do
      if nc -z ${KAFKA_BOOTSTRAP_HOSTNAME} ${KAFKA_PORT} ; then
        break
      elif [ $RETRY_COUNT -ne 200 ]; then
        ((RETRY_COUNT++))
        sleep 1s
      else
        echo "Could not connect to Kafka even after retries..."
        exit 1
      fi
    done
    java -jar ./consolidator.jar

# If a hub secret already exists give the secret name here. Else will be generated.
existingSecret: ""

# Give Hub Encryption Key here. If empty it will be autogenerated.
encryptionKey: ""

envVars:
  JDK_JAVA_OPTIONS: '{{ tpl .Values.javaOpts $ }}'
  HUB_PORT: '{{ .Values.containerPort }}'
  KAFKA_BOOTSTRAP_HOSTNAME: '{{ tpl .Values.kafkaInstallationName $ }}'
  KAFKA_PORT: '9092'
  KAFKA_BOOTSTRAP_SERVER: '{{ tpl .Values.envVars.KAFKA_BOOTSTRAP_HOSTNAME $ }}:{{ tpl .Values.envVars.KAFKA_PORT $ }}'
  CONSOLIDATOR_URL: 'http://{{ include "common.names.fullname" . }}-consolidator'
  CONSOLIDATOR_HEALTH_ENDPOINT: '/consolidator/actuator/health'
  SECURITY_JWT_ISSUER: '{{ tpl .Values.global.keycloakBaseUrl $ }}/realms/master'
  SECURITY_JWT_ISSUER_JWKS_URL: '{{ tpl .Values.envVars.SECURITY_JWT_ISSUER $ }}/protocol/openid-connect/certs'

envVarsFrom:
  hub_secret_encryption_key:
    secretKeyRef:
      name: '{{ (not .Values.existingSecret) | ternary (include "common.names.fullname" .) .Values.existingSecret }}'
      key: encryption-key

configTomlMountPath: /etc/config.template.toml

configToml: |-
  [kafkaHub.config]
  # Flag to check whether to enable/disable security
  SECURITY_ON = true

  # Security: JWT Issuer URL
  SECURITY_JWT_ISSUER = "${SECURITY_JWT_ISSUER}";

  # Security: JWKS Url for the configured issuer
  SECURITY_JWT_ISSUER_JWKS_URL = "${SECURITY_JWT_ISSUER_JWKS_URL}";

  # Server ID is is used to uniquely identify each server
  # Each server must have a unique ID
  SERVER_ID = "server-1"

  # IP and Port of the Kafka bootstrap node
  KAFKA_BOOTSTRAP_NODE = "${KAFKA_BOOTSTRAP_SERVER}"

  # Kafka topic which will get notified for websub topic registration/deregistration
  # All the hubs must be pointed to the same Kafka topic to notify websub topic registration/deregistration
  REGISTERED_WEBSUB_TOPICS_TOPIC = "registered-websub-topics"

  # Kafka topic which stores consolidated websub topics for the hub
  CONSOLIDATED_WEBSUB_TOPICS_TOPIC = "consolidated-websub-topics"

  # Kafka topic which will get notified for websub subscription/unsubscription
  # All the hubs must be pointed to the same Kafka topic to notify websub subscription/unsubscription
  WEBSUB_SUBSCRIBERS_TOPIC = "registered-websub-subscribers"

  # Kafka topic which is stores consolidated websub subscribers for this server
  CONSOLIDATED_WEBSUB_SUBSCRIBERS_TOPIC = "consolidated-websub-subscribers"

  # The interval in which Kafka consumers wait for new messages
  POLLING_INTERVAL = 10.0

  # The period in which Kafka close method waits to complete
  GRACEFUL_CLOSE_PERIOD = 5.0

  # The port that is used to start the hub
  HUB_PORT = ${HUB_PORT}

  # The period between retry requests
  MESSAGE_DELIVERY_RETRY_INTERVAL = 3.0

  # The maximum retry count
  MESSAGE_DELIVERY_COUNT = 3

  # The message delivery timeout
  MESSAGE_DELIVERY_TIMEOUT = 30.0

  # The token validation URL of IDP
  DISK_SPACE_THRESHOLD = 10485760

  # The token validation URL of IDP
  PARTNER_USER_ID_PREFIX = "service-account-"

  # The period between retry requests
  INTENT_VERIFICATION_RETRY_INTERVAL = 3.0

  # The maximum retry count
  INTENT_VERIFICATION_COUNT = 3

  # The period between retry requests
  INTENT_VERIFICATION_BACKOFF_FACTOR = 2.0

  # The maximum retry count
  INTENT_VERIFICATION_MAX_INTERVAL = 20.0

  # The maximum retry count
  KAFKA_CONSUMER_MAX_POLL_RECORDS = 3

  # The maximum retry count
  KAFKA_CONSUMER_FETCH_MAX_BYTES = 3145728

  # The maximum retry count
  KAFKA_CONSUMER_MAX_PARTITION_FETCH_BYTES = 524288

  # Kafka topic which is stores consolidated websub subscribers for this server
  META_TOPICS = "registered-websub-topics,consolidated-websub-topics,registered-websub-subscribers,consolidated-websub-subscribers"

  # consolidator base url
  CONSOLIDATOR_BASE_URL = "${CONSOLIDATOR_URL}"

  # consolidator health endpoint
  CONSOLIDATOR_HEALTH_ENDPOINT = "/consolidator/actuator/health"

  #Encryption key that will be used to encrypt / decrypt the hub secret
  HUB_SECRET_ENCRYPTION_KEY = "${hub_secret_encryption_key}"

  # Below config will allow base64-encoded-bytes / alpha-numeric.
  # Recommended to use base64-encoded-bytes since alpha-numeric is considered less secure. This is just given to ensure the backward compatiblity
  HUB_SECRET_ENCRYPTION_KEY_FORMAT = "alpha-numeric"

  [ballerina.http.accessLogConfig]
  # Enable printing access logs in console
  console = true              # Default is false

  [ballerina.http.traceLogAdvancedConfig]
  # Enable printing trace logs in console
  console = false              # Default is false

startUpCommand: |-
  #!/usr/bin/env bash
  echo "==> Copying Configs"
  envsubst < {{ tpl .Values.consolidator.configTomlMountPath $ }} > Config.toml
  echo "==> Waiting for kafka"
  RETRY_COUNT=0
  while :; do
    if nc -z ${KAFKA_BOOTSTRAP_HOSTNAME} ${KAFKA_PORT} ; then
      break
    elif [ $RETRY_COUNT -ne 200 ]; then
      ((RETRY_COUNT++))
      sleep 1s
    else
      echo "Could not connect to Kafka even after retries..."
      exit 1
    fi
  done
  echo "==> Waiting for consolidator"
  if ! curl -I -s -o /dev/null -m 10 --retry 100 --retry-delay 10 --retry-all-errors "${CONSOLIDATOR_URL}${CONSOLIDATOR_HEALTH_ENDPOINT}"; then
    echo "Failed connecting with consolidator after max retries..."
    exit 1
  fi
  java -jar ./hub.jar

kafkaInstallationName: '{{ include "common.names.fullname" .Subcharts.kafka }}'
